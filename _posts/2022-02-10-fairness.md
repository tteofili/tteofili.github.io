# Fairness assesments in black boxes
> A quick overview of different perspectives on evaluating fairness in opaque systems

[This survey on fairness](https://arxiv.org/pdf/1908.09635.pdf) and this newly released [book on Trustworthy ML](http://www.trustworthymachinelearning.com/trustworthymachinelearning.pdf) contain interesting information on bias and fairness when it comes to opaque systems (e.g., based on deep learning, machine learning, etc.).

## Bias

One first important bit to understand is what we mean by **bias** and from where it can originate.
The notion of **bias** has possibly different meanings depending on the context: e.g., in statistics this refers to a trend of deviating from the mean value for some variable; in machine learning with bias we refer to the happening of some unwanted or unexpected deviation in the behaviour of a trained model which can lead to unexpected outcomes.

To make this more concrete, we look at a different kinds of bias that can occur when data is used to train a ML model.

### Data --> Model :

  * _Measurement bias_ arises from how we choose, utilize, and measure particular features
  * _Omitted variable bias_ occurs when one or more important variables are left out of the model
  * _Representation bias_ arises from how we sample from a population during data collection process (lack of diversity in the data)
  * _Aggregation bias_ (or ecological fallacy) arises when false conclusions are drawn about individuals from observing the entire population
  * _Simpson’s paradox_ is a type of aggregation bias that arises in the analysis of heterogeneous data
  * _Sampling bias_ is similar to representation bias, and it arises due to nonrandom sampling of subgroups
  * _Linking bias_ arises when network attributes obtained from user connections, activities, or interactions differ and misrepresent the true behavior of the users

### Model --> User:
  * _Algorithmic bias_ when the bias is not present in the input data and is added purely by the algorithm
  * _Presentation bias_ is a result of how information is presented
  * _Ranking bias_: The idea that top-ranked results are the most relevant and important will result in attraction of more clicks than others
  * _Popularity Bias_: Items that are more popular tend to be exposed more. However, popularity metrics are subject to manipulation—for example, by fake reviews or social bots
  * _Emergent bias_ occurs as a result of use and interaction with real users, e.g., as a result of change in population, or societal knowledge usually some time after the completion of design
  * _Evaluation bias_ happens during model evaluation

### User --> Data:
  * Historical bias is the already existing bias and socio-technical issues in the world and can seep into from the data generation process even given a perfect sampling and feature selection
  * Population bias arises when statistics, demographics, representatives, and user characteristics are different in the user population of the platform from the original target population

## Fairness

Fairness is strightly bound to the concept of (political) justice, however what do we mean by justice? Does it exist one single justice?

In fact there are different perspectives to look at justice:
 * _Distributive justice_ is equality in what people receive—the outcomes
 * _Procedural justice_ is sameness in the way it is decided what people receive
 * _Restorative justice_ repairs a harm
 * _Retributive justice_ seeks to punish wrongdoers

Let's think of retributive justice as an example, is it fair if all wrongdoers are punished the same way?
When it comes to what those individuals might have done wrong, there're might be differences in how much wrong they did.
Also, are there any differences among such wrongdoers? Are mean and women treated equally in this regard?
If we expect that all wrongdoers should be equally punished, regardless for exampler of their gender, we are demanding _group fairness_.
Taking it one step further, we can demand that similar wrongdoers (e.g. people that parked their car in prohibited area) should be punished equally, hence demanding _individual fairness_.


In fairness we want to make sure that sensitive or "protected" attributes like age, gender, religion, etc. are not source of discrimination. Of course there is no one universal set of protected attributes. Discrimination happens when for example some groups or individuals are given some _privilege_, in the case of ML this results in getting a "favorable" label with higher likelihood.
Privilege is a result of power imbalances, and the same groups may not be privileged in all contexts.

Group fairness is the idea that the average classifier behavior should be the same across groups defined by protected attributes.
Individual fairness includes the special case of two individuals who are exactly the same in every respect except for the value of one protected attribute
(this special case is known as counterfactual fairness).
Individual fairness is the idea that individuals similar in their features should receive similar model predictions.

Bias in measurement and sampling are the most obvious sources of unfairness in machine learning, but not the only ones.

When it comes to machine learning, a very easy way to read fairness, e.g., in the context of classification tasks, is that protected and unprotected groups should have equal rates for true positives and false positives.

Other ways to look at fairness areg:
 * _Fairness Through Awareness_: An algorithm is fair if it gives similar predictions to similar individuals
 * _Fairness Through Unawareness_: An algorithm is fair as long as any protected attributes A are not explicitly used in the decision-making process
 * _Counterfactual Fairness_: a decision is fair towards an individual if it is the same in both the actual world and a counterfactual world where the individual belonged to a different demographic group


## Metrics for group fairness

### Statistical parity difference (SPD)

Statiscial/demographic parity difference is calcualted as the probability that unprivileged group is assigned a favorable label minus probability that privileged group is assigned a favorable label.

### Disparate impact ratio (DI)

Disparate impact ratio (aka relative risk ratio or adverse impact ratio) is calculated very similar to SPD but as the fraction between the probability that unprivileged group is assigned a favorable label and the probability that privileged group is assigned a favorable label.

Statistical parity difference and disparate impact ratio can be understood as measuring a form of independence between a prediction and the protected attribute.

Both statistical parity difference and disparate impact ratio can also be defined on the training data instead of the model predictions by replacing predictions with actual labels. Thus, they can be measured and tested (1) on the dataset before model training, as a dataset fairness metric, as well as (2) on the learned classifier after model training as a classifier fairness metric.

### Average odds difference (AOD)

Average odds difference is based on model performance metrics and is calculated as the (averaged) difference of true favorable rates between the unprivileged and privileged groups and the difference of the false favorable rates between the unprivileged and privileged groups.

```python
((tp/(tp+fn))[unpriv] - (tp/(tp+fn))[priv])/2 + ((fp/(fp+tn))[unpriv] - (fp/(fp+tn))[priv])/2
```

### Calibration by group (or sufficiency)

```python
((tp/(tp+fn))[unpriv] - (tp/(tp+fn))[priv])/2 + ((fn/(fn+tn))[unpriv] - (fn/(fn+tn))[priv])/2
```

### Consistency for individual fairness

Given a dataset _X_ of size _n_, a model _f_ a proximity function _p(x)_ that generates the "neighborhood" of an instance _x_, consistency is calculated as:
```python

consitency = 1
for x in X:
 neighbors = pi(x)
 for b in neighbors:
  consistency -= (f(x) - f(b))/n

```

