# Fairness assesments in black boxes
> A quick overview of different perspectives on evaluating fairness in opaque systems

[This survey on fairness](https://arxiv.org/pdf/1908.09635.pdf) contains interesting information on different ways to evaluate fairness for opaque systems (e.g., based on deep learning, machine learning, etc.).

## Data -> Algorithm :

  * measurement bias arises from how we choose, utilize, and measure particular features
  * Omitted variable bias occurs when one or more important variables are left out of the model
  * Representation bias arises from how we sample from a population during data collection process (lack of diversity in the data)
  * Aggregation bias (or ecological fallacy) arises when false conclusions are drawn about individuals from observing the entire population
  * Simpson’s paradox is a type of aggregation bias that arises in the analysis of heterogeneous data
  * Sampling bias is similar to representation bias, and it arises due to nonrandom sampling of subgroups
  * Linking bias arises when network attributes obtained from user connections, activities, or interactions differ and misrepresent the true behavior of the users

## Algorithm -> User:
  * algorithmic bias when the bias is not present in the input data and is added purely by the algorithm
  * Presentation bias is a result of how information is presented
  * Ranking bias: The idea that top-ranked results are the most relevant and important will result in attraction of more clicks than others
  * Popularity Bias. Items that are more popular tend to be exposed more. However, popularity metrics are subject to manipulation—for example, by fake reviews or social bots
  * Emergent bias occurs as a result of use and interaction with real users, e.g., as a result of change in population, or societal knowledge usually some time after the completion of design
  * Evaluation bias happens during model evaluation

## User -> Data:
  * Historical bias is the already existing bias and socio-technical issues in the world and can seep into from the data generation process even given a perfect sampling and feature selection
  * Population bias arises when statistics, demographics, representatives, and user characteristics are different in the user population of the platform from the original target population

  * Fairness: protected and unprotected groups should have equal rates for true positives and false positives
    * Fairness Through Awareness: An algorithm is fair if it gives similar predictions to similar individuals
    * Fairness Through Unawareness: An algorithm is fair as long as any protected attributes A are not explicitly used in the decision-making process
    * Counterfactual Fairness: a decision is fair towards an individual if it is the same in both the actual world and a counterfactual world where the individual belonged to a different demographic group
    * Individual Fairness. Give similar predictions to similar individuals
    * Group Fairness. Treat different groups equally
    * Subgroup Fairness. Subgroup fairness intends to obtain the best properties of the group and individual notions of fairness

