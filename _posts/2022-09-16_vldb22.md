# Interesting stuff @ VLDB 2022
> Interesting stuff from [VLDB 2022](https://vldb.org/2022) conference in Sydney.

## The unreasonable effectiveness (?) of ML for anything-database related

Machine learning is being used in nearly every possible way for query processing: like for example in [parametric query optimization](https://www.vldb.org/pvldb/vol15/p401-vaidya.pdf) but also to perform query cost prediction using [zero shot learning](https://www.vldb.org/pvldb/vol15/p2361-hilprecht.pdf) or estimating the number of distinct values [using supervised learning](https://www.vldb.org/pvldb/vol15/p272-wu.pdf). 
To some extent, some typical such tasks are becoming natural language processing tasks, as an example there's an interesting work to efficiently perform cardinality estimation by [pre-training summarization models of structured datasets](https://www.vldb.org/pvldb/vol15/p414-lu.pdf); other proposed alternatives for performing cardinality estimation rely on machine learning techniques, see [Learned Cardinality Estimation: A Design Space Exploration and
A Comparative Evaluation](https://vldb.org/pvldb/vol15/p85-li.pdf) and [Cardinality Estimation of Approximate Substring Queries using
Deep Learning](https://vldb.org/pvldb/vol15/p85-li.pdf)[^1].

More generally database workload characterization is also being addressed by means of [deep learning techniques](https://www.vldb.org/pvldb/vol15/p923-paul.pdf) or [reinforcement learning](https://vldb.org/pvldb/vol14/p3402-wang.pdf).

_Learned_ indexes seem to be very popular and are reported to be replacing common physical indexes, e.g. [recursive model indexes](https://www.vldb.org/pvldb/vol15/p1079-maltry.pdf).
An interesting work is the one that transforms SQL queries into [tensor programs](https://www.vldb.org/pvldb/vol15/p2811-he.pdf)).

As the ML wave keeps rising, new database systems which run (fully or partially) over neural networks keeps being created, e.g. for performing [spatial range queries](https://www.vldb.org/pvldb/vol15/p1066-zeighami.pdf) (with differential privacy guarantees).

Other tasks where ML is adopted involve designing storage optimizations on databases systems like [model serving of deep learning models](https://www.vldb.org/pvldb/vol15/p2230-zou.pdf) or [selecting the best storage settings via machine learning](https://www.vldb.org/pvldb/vol15/p3126-abebe.pdf).

While on one hand this is all very nice and interesting (especially if you like to work with machine learning), it also seems that some works presented at VLDB this year might have been submitted (also) to some more generic ML venues like ICML, NeurIPS, AAAI, etc. 
For example the interesting work [ByteGNN: Efficient Graph Neural Network Training at Large Scale](https://www.vldb.org/pvldb/vol15/p1228-zheng.pdf) seems a quite generic (and good) one that seems a bit too much "just-ML" oriented for a database/data management kind of conference.


## Explainability

_Influence functions_ are a very popular [explainability tool](https://proceedings.mlr.press/v70/koh17a.html), designed to discover which training data point are most influencing for a specific prediction output. The work [Efficient and Effective Data Imputation with Influence Functions](https://www.vldb.org/pvldb/vol15/p624-miao.pdf) adapts them to the data management use cases and scenario.

[Analyzing How BERT Performs Entity Matching](https://www.vldb.org/pvldb/vol15/p1726-paganelli.pdf), this work analyses how BERT based models for the _entity resolution_ task performs its predictions. An interesting insight is that _"the pair-wise semantic similarity of tokens is not a key knowledge exploited by BERT-based EM models"_; this finding, while somewhat expected if you have ever worked with one of those models, might sound surprising since this is counterintuitive with respect to how humans typically think to the entity matching task (semantic similarity between aligned pairs of text sounds important to decide if two records refer to the same entity).

The work title [DeepEverest: Accelerating Declarative Top-K Qeries for Deep Neural Network Interpretation](https://www.vldb.org/pvldb/vol15/p98-he.pdf) describes a system for the efficient execution of "interpretation by example" queries over the activation values of a deep neural network. This basically speeds up white box explanation techniques for deep neural networks that "look" at neurons' activations.

Despite not strictly dealing with ExplainableAI, the paper [On Shapley Value in Data Assemblage Under Independent Utility](https://www.vldb.org/pvldb/vol15/p2761-luo.pdf) provides an interesting take on evaluating the quality of data by means of Shapley fairness under the independent utility assumption.

## Best paper (awarded)

These are the best papers selected by the conference committee:

[Sancus: Staleness-Aware Communication-Avoiding Full-Graph Decentralized Training in Large-Scale Graph Neural Networks](https://vldb.org/pvldb/vol15/p1937-peng.pdf) (best regular research paper).

[HET: Scaling out Huge Embedding Model Training via Cache-enabled Distributed Framework](https://www.vldb.org/pvldb/vol15/p312-miao.pdf) (best scalabale data science paper).

[Threshold Queries in Theory and in the Wild](https://www.vldb.org/pvldb/vol15/p1105-staworko.pdf) (best regular research paper runner ups).

[Sortledton: a universal, transactional graph data structure](https://www.vldb.org/pvldb/vol15/p1173-fuchs.pdf) (best regular research paper runner ups).

[Accurate Summary-based Cardinality Estimation Through the Lens of Cardinality Estimation Graphs](https://www.vldb.org/pvldb/vol15/p1533-chen.pdf) (best experiment, analysis and benchmark paper).

[Hardware Acceleration of Compression and Encryption in SAP HANA](https://www.vldb.org/pvldb/vol15/p3277-chiosa.pdf) (best industry paper).

## Interesting papers (imho)

While very intuitive and simple, I have found the work [Ember: No-Code Context Enrichment via Similarity-Based Keyless Joins](https://www.vldb.org/pvldb/vol15/p699-suri.pdf) as the most elegant and best presented paper in the whole conference. _Ember_ abstracts
and automates keyless joins to generalize context enrichment by means of a _keyless join_ operator by constructing an index populated with task-specific embeddings by leveraging Transformer-based representation learning techniques.

Also very nice is the paper [Entity Resolution On-Demand](https://www.vldb.org/pvldb/vol15/p1506-simonini.pdf) introduces _BrewER_, a framework to evaluate SQL queries on dirty data while progressively returning results as if they were issued on cleaned data. The interesting aspect of _BrewER_ is that it tries to focus on the cleaning effort on one entity at a time following an _ORDER BY_ predicate, saving a significant amount of resources.
In addition to [BrewER](https://www.vldb.org/pvldb/vol15/p1506-simonini.pdf), other interesting papers for the entity resolution task: the work
[Deep Transfer Learning for Multi-source Entity Linkage via Domain Adaptation](https://www.vldb.org/pvldb/vol15/p465-jin.pdf) uses deep learning and domain adaptation to effectively perform ER, while the work [Deep Indexed Active Learning for Matching Heterogeneous Entity Representations](https://www.vldb.org/pvldb/vol15/p31-jain.pdf) leverages active learning for low resource entity resolution settings.

Since a while, dense retrieval is a very hot topic, especially in the Information Retrieval community, hence the work [LANNS: A Web-Scale Approximate Nearest Neighbor Lookup System](https://www.vldb.org/pvldb/vol15/p850-doshi.pdf) seems an interesting solution for scaling the (now) common HNSW algorithm for performing approximate nearest neighbor on large datasets.

## Sydney

Sydney is a modern beautiful city, at least for what concerns downtown and surroundings (Haymarket, Darling Harbour, Pott's point, etc.), as far as I could see. People is generally kind, streets are clean, full of people and people at work.
There's plenty of places to take a nice walk: the harbours and docks, the Botanic Garden, the Opera House, and more.

[^1]:Back in 2015 I would have never imagined that it might be such a good idea to [learn](https://github.com/apache/jackrabbit-oak/commits/5c9a4be46318139eb7c311feaf10a8daa160dac7/oak-solr-core/src/main/java/org/apache/jackrabbit/oak/plugins/index/solr/query/LMSEstimator.java?browsing_rename_history=true&new_path=oak-search/src/main/java/org/apache/jackrabbit/oak/plugins/index/search/util/LMSEstimator.java&original_branch=5c9a4be46318139eb7c311feaf10a8daa160dac7) to perform cost estimation within a query engine (well, least mean square regression, I know).
