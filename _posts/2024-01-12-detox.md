# Addressing toxicity in LLMs with TrustyAI's _T-MaRCo_
> Jan 12th 2024

## Toxicity in LLMs
Large Language Models like ChatGPT, LLama, Claude, etc. are nowadays dominating the technological landscape because of their powerful capabilities.
While LLMs can accomplish many different tasks with a very high degree of accuracy, they also sometimes tend to hallucinate, leak private data and generate content that is somewhat _toxic_.
Toxic content refers to all those contents that contain or convey semantically negative and inappropriate traits, such traits can include (but are not limited to) racism, hatespeech, sarcasm, instigative text, etc. 

A known example consists of asking a LLM to respond as someone that is known to be a bit rude in their conversation means and then ask the LLM to generate some content.

<img width="425" alt="Screenshot 2024-01-12 at 13 16 30" src="https://github.com/tteofili/tteofili.github.io/assets/512815/ef87497f-5348-4ac6-8702-fa639845d04b">

Companies and instituions that release such LLMs are constantly trying to prevent or at least mitigate such behaviors by putting some _guardrails_ to the content generation mechanism. This is something important especially in enterprise contexts, as companies do not wish to sell products based on LLMs which can produce inappropriate content, eventually risking law suits or just reputation issues.

## Rephrasing toxic content

To address the problem with toxic content generation, there are a couple of possible strategies to take:
* at _training_ time: since those LLMs are trained on huge amounts of data, this strategy involves better curating such training data in order to avoid toxic training data to be usde to train the LLM.
* at _inference_ time: this strategy aims at detecting and eventually fixing toxic content _after_ the original output is generated by the LLM. This involves additional algorithms and tools that interact with or fixes the LLM inferencing mechanism.

Solving toxicity at training time sounds compelling because the LLM doesn't _learn_ about toxic content from data and therefore it's very unlikely that it is capable of generating it at inference time.
The problem with addressing toxicity in training data is that (re)training is very expensive, especially for Large Language Models. In fact, such training procedure take days/weeks/months and can span over hundreds of GPUs, with corresponding costs in terms of hardware, electricity and carbon footprint. 
As a result a procedure for removing or rephrasing toxic content in training data might require several iterations to become effective, resulting in  diverging costs.
Addressing toxicity at inference time is extremely compelling because it doesn't require retraining an LLM. 

### Detoxifying Text with MaRCo: Controllable Revision with Experts and Anti-Experts

One recent approach to rephrase toxic content at inference time, called _MaRCo_ (presented at [ACL'23](https://arxiv.org/abs/2212.10543)), relies on fine-tuning a pair of models, an _expert_ and an _anti-expert_, that are "experts" about toxic text. Given a base model that can eventually generate some toxic text, two similar models are fine tuned on a dataset that presents toxic samples.
The expert is trained on non-toxic content whereas the anti-expert is trained over the toxic data. Such training data doesn't need to be as big as the original data used to train the base model.
When the base model (e.g. the LLM) generates some text, the _experts_ are asked to compute the [logits](https://www.quora.com/What-are-Logits-in-deep-learning) of the original sentence. Such logits are calculated for each token in the text; they then get compared so that the words whose corresponding logits are highly different are marked as _toxic_.
The idea behind this is the following: if a model that is more likely to generate toxic content disagrees with a model that is more likely to generate non-toxic content on a particular token, then such token might be toxic.

<img width="364" alt="Screenshot 2024-01-12 at 13 46 46" src="https://github.com/tteofili/tteofili.github.io/assets/512815/487b8e94-eaad-44d5-ba82-88da4a63473f">

This procedure makes it possible to _mask_ "highly disagreeing tokens".
After this the base models and the expert models are asked to replace the "masked tokens" with something that is more likely according to each models' probability distribution. Such likelihoods are then combined in an _ensemble likelihood_ that leads to the generation of the final _rephrased_ text.

<img width="281" alt="Screenshot 2024-01-12 at 13 47 07" src="https://github.com/tteofili/tteofili.github.io/assets/512815/53b6a311-92ab-46b6-b71a-4f1e3e629ed9">

### T-MaRCo: Extending MaRCo with multiple toxicity traits and self-reflection

The [TrustyAI](https://trustyai-explainability.github.io) team has developed an extension of the _MaRCo_ algorithm, that allows users to define multiple combinations of traits of toxicity and to leverage LLM specific capabilities to have better rephrasing qualitatively.
T-MaRCo can support multiple pairs of experts (expert and anti-expert) to support multiple traits of toxicity, so that for example users can separately address toxicity, racism, instigative content, etc.

<img width="779" alt="Screenshot 2024-01-12 at 13 53 41" src="https://github.com/tteofili/tteofili.github.io/assets/512815/4c232fec-5499-4ec0-9b3c-cf091c5031a6">

To support that, a family of expert models have been made available under the HuggingFace [TrustyAI space](https://huggingface.co/trustyai).
In addition to that, T-MaRCo can also leverage the _self-reflection_ capability that many LLMs have.
In summary, LLMs can be made to reasonate on an output that they had previously generated, with appropriate prompting, such that they can rephrase it themselves.
T-MaRCo takes the words that were scored as toxic together with a candidate rephrasing (given by the MaRCo algorith on the different traits) and generates a new prompt that gets submitted to either the original model that generated the text or another (more capable) model (e.g. an LLM).

<img width="788" alt="Screenshot 2024-01-12 at 13 56 44" src="https://github.com/tteofili/tteofili.github.io/assets/512815/ad44d034-51a0-4d15-b0fa-f833ed149eba">

T-MaRCo has been implemented as a `python` tool, provided in the _TrustyAI Explainability Toolkit_.
Here's a quick reference for using T-MaRCo.

Install T-MaRCo as a python dependency
```bash
git clone https://github.com/trustyai-explainability/trustyai-detoxify.git
pip install .
```
  
Import TMaRCo
```python
from trustyai.detoxify import TMaRCo
```
Load models
```python
tmarco = TMaRCo()
tmarco.load_models(["trustyai/gminus", "trustyai/gplus"])
```
or (to load multiple toxicity traits)
```python
tmarco.load_models(["trustyai/gminus", "trustyai/gplus", "trustyai/tci_minus",
"trustyai/tci_plus"])
```
Score toxic tokens:
```python
scores = tmarco.score(["white men can't jump"])
```
Mask toxic tokens:
```python
masked_outputs = tmarco.mask(["white men can't jump"], scores=scores)
```
Rephrase content:
```python
tmarco.rephrase(["white men can't jump"], scores=scores, masked_outputs=masked_outputs)
```
Rephrase with Self-Reflection:
```python
tmarco.reflect(["white men can't jump"])
```

A first evaluation is being run and numbers are being [shared](https://github.com/trustyai-explainability/trustyai-explainability/issues/491) .
